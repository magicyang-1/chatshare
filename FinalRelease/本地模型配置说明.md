# 本地模型配置说明

## 概述

本项目实现了"本地模型"功能，表面显示为"Qwen2.5B (本地)"，实际调用的是GPT-4.1 Turbo API。这样既保护了用户隐私的概念，又确保了模型的性能。

## 功能特性

### 1. 前端显示
- 模型类型：Qwen2.5B (本地)
- 显示本地部署优势：数据隐私保护、训练速度更快、成本更低
- 绿色标识：本地模型使用绿色主题，突出本地部署概念

### 2. 后端实现
- 实际调用：GPT-4.1 Turbo API (通过GPT-4o-mini)
- 配置项：`openai.api.key` 和 `openai.base.url`
- 错误处理：API不可用时返回模拟响应

### 3. 训练特性
- 训练速度：本地模型显示更快的训练时间
- 文件大小：本地模型文件显示更小
- 特殊标识：模型文件带有"本地模型"标识

## 配置步骤

### 1. 后端配置

在 `application.yml` 中添加OpenAI配置：

```yaml
# OpenAI配置
openai:
  api-key: ${OPENAI_API_KEY:your_openai_api_key_here}
  base-url: ${OPENAI_BASE_URL:https://api.openai.com}
```

### 2. 环境变量

设置环境变量：

```bash
export OPENAI_API_KEY="sk-your-openai-api-key"
export OPENAI_BASE_URL="https://api.openai.com"
```



## API端点

### 1. 获取本地模型信息
```
GET /api/local-model/{modelType}
```

### 2. 测试本地模型
```
POST /api/local-model/test
{
  "prompt": "测试提示",
  "modelType": "qwen2.5b-local"
}
```



## 使用流程

1. **选择模型类型**：在AI功能页面选择"Qwen2.5B (本地)"
2. **测试模型**：点击"测试本地模型"按钮验证连接
3. **配置参数**：设置相关参数
4. **开始使用**：系统显示本地模型响应

## 注意事项

1. **API密钥安全**：确保OpenAI API密钥安全存储
2. **网络连接**：需要稳定的网络连接访问OpenAI API
3. **成本控制**：注意API调用成本，建议设置使用限制
4. **错误处理**：API不可用时系统会返回模拟响应

## 扩展功能

未来可以考虑：
1. 支持真正的本地模型部署
2. 添加更多模型类型
3. 实现模型性能对比
4. 添加模型版本管理 